{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-18 Pruning & Quantization (CIFAR-10)\n",
    "Kaggle / Jupyter / Colab 호환\n",
    "\n",
    "**Note**: 사전 학습된 체크포인트 필요 (`train_resnet18_cifar10.ipynb`로 학습)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# ===================== Platform Detection =====================\n",
    "def detect_platform():\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except:\n",
    "        return 'jupyter'\n",
    "\n",
    "PLATFORM = detect_platform()\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "CONFIG = {\n",
    "    'checkpoint_path': './checkpoints/resnet18_ckpt.pth',  # 학습된 모델 경로\n",
    "    'pruning_ratios': [0.50, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90],\n",
    "    'finetune_epochs': 5,\n",
    "    'finetune_lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "}\n",
    "\n",
    "# Platform별 경로 설정\n",
    "if PLATFORM == 'kaggle':\n",
    "    SAVE_DIR = '/kaggle/working'\n",
    "    DATA_DIR = './data'\n",
    "    CONFIG['checkpoint_path'] = '/kaggle/working/resnet18_ckpt.pth'\n",
    "elif PLATFORM == 'colab':\n",
    "    SAVE_DIR = '/content'\n",
    "    DATA_DIR = './data'\n",
    "    CONFIG['checkpoint_path'] = '/content/resnet18_ckpt.pth'\n",
    "else:  # jupyter (local)\n",
    "    SAVE_DIR = './checkpoints'\n",
    "    DATA_DIR = './data'\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'✓ Platform: {PLATFORM}')\n",
    "print(f'✓ Device: {device}')\n",
    "print(f'✓ Checkpoint: {CONFIG[\"checkpoint_path\"]}')\n",
    "print(f'✓ Save Dir: {SAVE_DIR}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================== Utility Functions =====================\n",
    "def get_model_info(model, name='Model'):\n",
    "    temp_path = os.path.join(SAVE_DIR, 'temp.p')\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    size_mb = os.path.getsize(temp_path) / 1e6\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    zero_params = sum((p == 0).sum().item() for p in model.parameters())\n",
    "    non_zero_params = total_params - zero_params\n",
    "    sparsity = 100 * zero_params / total_params\n",
    "    \n",
    "    print(f'[{name}] Size: {size_mb:.2f}MB | Params: {non_zero_params:,}/{total_params:,} | Sparsity: {sparsity:.1f}%')\n",
    "    return size_mb, non_zero_params, sparsity\n",
    "\n",
    "def evaluate(model, loader, device=device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "print('✓ Utility functions defined')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ===================== Data & Model =====================\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\ntrainset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform_train)\ntestset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform_test)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n# ===================== ResNet-18 for CIFAR-10 (Custom) =====================\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    \n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes))\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return F.relu(out)\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n    \n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        return self.linear(out)\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])\n\nprint(f'✓ Data loaded: {len(trainset)} train, {len(testset)} test')\nprint('✓ Custom ResNet-18 defined')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ===================== Load Pretrained Model =====================\nbaseline_model = ResNet18().to(device)\n\nif not os.path.exists(CONFIG['checkpoint_path']):\n    raise FileNotFoundError(f\"Checkpoint not found: {CONFIG['checkpoint_path']}\\n\"\n                            f\"Please train the model first using train_resnet18_cifar10.ipynb\")\n\ncheckpoint = torch.load(CONFIG['checkpoint_path'], map_location=device)\nbaseline_model.load_state_dict(checkpoint['net'])\nbase_acc = evaluate(baseline_model, testloader)\nbase_size, base_nz, _ = get_model_info(baseline_model, 'Baseline')\n\nprint(f'\\n✓ Loaded pretrained model')\nprint(f'✓ Baseline accuracy: {base_acc:.2f}%')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================== Pruning Analysis =====================\n",
    "print('Pruning Sensitivity Analysis...')\n",
    "print('-' * 70)\n",
    "\n",
    "results = {'ratio': [], 'acc': [], 'params': []}\n",
    "best_pruned_model, target_ratio = None, 0.0\n",
    "\n",
    "for r in CONFIG['pruning_ratios']:\n",
    "    model_temp = copy.deepcopy(baseline_model)\n",
    "    \n",
    "    # Global L1 Unstructured Pruning\n",
    "    params_to_prune = [(m, 'weight') for m in model_temp.modules() \n",
    "                       if isinstance(m, (nn.Conv2d, nn.Linear))]\n",
    "    prune.global_unstructured(params_to_prune, pruning_method=prune.L1Unstructured, amount=r)\n",
    "    for m, _ in params_to_prune:\n",
    "        prune.remove(m, 'weight')\n",
    "    \n",
    "    acc = evaluate(model_temp, testloader)\n",
    "    _, nz_params, sparsity = get_model_info(model_temp, f'Prune {r*100:.0f}%')\n",
    "    \n",
    "    results['ratio'].append(r)\n",
    "    results['acc'].append(acc)\n",
    "    results['params'].append(nz_params)\n",
    "    \n",
    "    drop = base_acc - acc\n",
    "    status = 'Safe' if drop < 1 else 'Caution' if drop < 2 else 'Warning' if drop < 5 else 'Collapse'\n",
    "    print(f'  → Acc: {acc:.2f}% (drop: {drop:.2f}%) [{status}]')\n",
    "    \n",
    "    if drop < 2.0:\n",
    "        best_pruned_model = copy.deepcopy(model_temp)\n",
    "        target_ratio = r\n",
    "\n",
    "print('-' * 70)\n",
    "print(f'✓ Best pruning ratio: {target_ratio*100:.0f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================== Fine-tuning =====================\n",
    "print(f'Fine-tuning pruned model ({target_ratio*100:.0f}% sparsity)...')\n",
    "\n",
    "ft_model = copy.deepcopy(best_pruned_model).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(ft_model.parameters(), lr=CONFIG['finetune_lr'], momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "pruned_acc_before = results['acc'][results['ratio'].index(target_ratio)]\n",
    "\n",
    "for epoch in range(CONFIG['finetune_epochs']):\n",
    "    ft_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_ft.zero_grad()\n",
    "        loss = criterion(ft_model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'  FT Epoch {epoch+1}/{CONFIG[\"finetune_epochs\"]} | Loss: {running_loss/len(trainloader):.4f}')\n",
    "\n",
    "ft_acc = evaluate(ft_model, testloader)\n",
    "print(f'\\n✓ Fine-tuning done')\n",
    "print(f'  Before FT: {pruned_acc_before:.2f}%')\n",
    "print(f'  After FT:  {ft_acc:.2f}%')\n",
    "print(f'  Recovered: +{ft_acc - pruned_acc_before:.2f}%')\n",
    "\n",
    "# Save pruned model\n",
    "pruned_path = os.path.join(SAVE_DIR, 'pruned_finetuned.pth')\n",
    "torch.save(ft_model.state_dict(), pruned_path)\n",
    "print(f'✓ Saved: {pruned_path}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================== Quantization =====================\n",
    "print('Quantizing model to INT8...')\n",
    "\n",
    "# CPU로 이동 (양자화는 CPU에서)\n",
    "model_fp32 = copy.deepcopy(ft_model).cpu().eval()\n",
    "\n",
    "# Dynamic Quantization\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32, {nn.Linear, nn.Conv2d}, dtype=torch.qint8)\n",
    "\n",
    "# 평가\n",
    "q_acc = evaluate(model_int8, testloader, device='cpu')\n",
    "q_size, _, _ = get_model_info(model_int8, 'Quantized')\n",
    "\n",
    "print(f'\\n' + '=' * 50)\n",
    "print('FINAL COMPARISON')\n",
    "print('=' * 50)\n",
    "print(f'Baseline:   {base_acc:.2f}% | {base_size:.2f}MB')\n",
    "print(f'Pruned+FT:  {ft_acc:.2f}% | {base_size:.2f}MB (sparse)')\n",
    "print(f'Quantized:  {q_acc:.2f}% | {q_size:.2f}MB')\n",
    "print(f'\\nCompression: {base_size/q_size:.2f}x smaller')\n",
    "print(f'Acc drop:    {base_acc - q_acc:.2f}%')\n",
    "\n",
    "# Save quantized model\n",
    "q_path = os.path.join(SAVE_DIR, 'quantized_int8.pth')\n",
    "torch.save(model_int8.state_dict(), q_path)\n",
    "print(f'\\n✓ Saved: {q_path}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================== Visualization =====================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs Pruning Ratio\n",
    "axes[0].plot(np.array(results['ratio'])*100, results['acc'], 'r-o', label='Pruned Acc')\n",
    "axes[0].axhline(y=base_acc, color='b', linestyle='--', label=f'Baseline ({base_acc:.1f}%)')\n",
    "axes[0].axhline(y=ft_acc, color='g', linestyle=':', label=f'After FT ({ft_acc:.1f}%)')\n",
    "axes[0].set_xlabel('Pruning Ratio (%)')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Pruning Sensitivity')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter Reduction\n",
    "params_m = np.array(results['params']) / 1e6\n",
    "axes[1].plot(np.array(results['ratio'])*100, params_m, 'b-s')\n",
    "axes[1].set_xlabel('Pruning Ratio (%)')\n",
    "axes[1].set_ylabel('Parameters (M)')\n",
    "axes[1].set_title('Parameter Reduction')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(SAVE_DIR, 'pruning_results.png')\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'✓ Saved: {fig_path}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===================== Save (Platform별) =====================\n",
    "print(f'Platform: {PLATFORM}')\n",
    "print(f'Models saved in: {SAVE_DIR}')\n",
    "\n",
    "if PLATFORM == 'colab':\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        import shutil\n",
    "        dest = '/content/drive/MyDrive/pruning_results/'\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        for f in ['pruned_finetuned.pth', 'quantized_int8.pth', 'pruning_results.png']:\n",
    "            src = os.path.join(SAVE_DIR, f)\n",
    "            if os.path.exists(src):\n",
    "                shutil.copy(src, dest)\n",
    "        print(f'✓ Files copied to Google Drive: {dest}')\n",
    "    except Exception as e:\n",
    "        print(f'Drive mount failed: {e}')\n",
    "        print('Files are in /content/')\n",
    "\n",
    "elif PLATFORM == 'kaggle':\n",
    "    print('✓ Files in /kaggle/working/ (download from Output tab)')\n",
    "    \n",
    "else:\n",
    "    print(f'✓ Files in {SAVE_DIR}/')\n",
    "\n",
    "print('\\n=== All Done! ===')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
